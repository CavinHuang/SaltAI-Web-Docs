---
tags:
- LLM
---

# âˆž Complete Query
## Documentation
- Class name: `LLMComplete`
- Category: `SALT/Llama-Index/Querying`
- Output node: `False`

The LLMComplete node is designed to generate text completions using a specified language model. It takes a prompt as input and returns the model's completion of that prompt, showcasing its ability to extend or complete given text sequences based on the model's training.
## Input types
### Required
- **`llm_model`**
    - Specifies the language model to use for generating text completions. This parameter is crucial as it determines the behavior and capabilities of the completion process.
    - Comfy dtype: `LLM_MODEL`
    - Python dtype: `Dict[str, Any]`
- **`prompt`**
    - The initial text prompt to which the language model will generate a completion. This input is essential for guiding the model's output in a specific direction or context.
    - Comfy dtype: `STRING`
    - Python dtype: `str`
## Output types
- **`completion`**
    - Comfy dtype: `STRING`
    - The text generated by the language model as a completion to the provided prompt.
    - Python dtype: `str`
## Usage tips
- Infra type: `GPU`
- Common nodes: unknown


## Source code
```python
class LLMComplete:
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "llm_model": ("LLM_MODEL", ),
                "prompt": ("STRING", {"multiline": True, "dynamicPrompts": False, "placeholder": "The circumference of the Earth is"}),
            },
        }

    RETURN_TYPES = ("STRING", )
    RETURN_NAMES = ("completion", )

    FUNCTION = "complete"
    CATEGORY = "SALT/Llama-Index/Querying"

    def complete(self, llm_model:Dict[str, Any], prompt:str) -> str:
        response = llm_model['llm'].complete(prompt)
        pprint(response, indent=4)
        return (response.text, )

```
